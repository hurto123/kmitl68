# -*- coding: utf-8 -*-
"""
Legal AI RAG Engine
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö RAG (Retrieval-Augmented Generation)

üèÜ CONTRIBUTION: Main RAG Engine that forces LLM to use document context
"""

from typing import List, Tuple, Optional
from pathlib import Path

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.config import NUM_RELEVANT_DOCS
from llm.ollama_client import OllamaClient, get_ollama_client
from llm.prompt_templates import (
    BASIC_QA_PROMPT,
    LEGAL_SUMMARY_PROMPT,
    THAI_LEGAL_PROMPT,
    NO_CONTEXT_PROMPT,
    LOW_RELEVANCE_PROMPT,
    SYSTEM_PROMPT_DISCLAIMER,
    get_prompt_template,
    format_response_with_disclaimer
)
from ingestion.file_loader import FileLoader
from vector_store.vector_db_manager import VectorDBManager, get_vector_db
from vector_store.retriever import LegalRetriever, get_retriever


class LegalAIEngine:
    """
    RAG Engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
    
    üéØ ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö:
    - ‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô vectors ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô ChromaDB
    - ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ context ‚Üí ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° context
    - LLM ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å context ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡πÄ‡∏î‡∏≤)
    
    Features:
    - Document ingestion pipeline
    - RAG chat function
    - Document summarization
    - Source tracking
    """
    
    _instance: Optional['LegalAIEngine'] = None
    
    def __new__(cls):
        """Singleton pattern"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        # Initialize components
        self.file_loader = FileLoader()
        self.vector_db = get_vector_db()
        self.retriever = get_retriever(self.vector_db)
        self.llm_client = get_ollama_client()
        self.llm = self.llm_client.get_llm()
        
        self._initialized = True
    
    # =========================================================================
    # Document Ingestion
    # =========================================================================
    
    def ingest_file(self, file_path: str) -> dict:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: Upload ‚Üí Load ‚Üí Split ‚Üí Embed ‚Üí Store
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            
        Returns:
            dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        """
        try:
            # 1. Process file (load, split, clean)
            result = self.file_loader.process_file(file_path)
            
            if not result["success"]:
                return result
            
            # 2. Add to vector database
            chunks = result["chunks"]
            num_added = self.vector_db.add_documents(chunks)
            
            return {
                "success": True,
                "message": f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {result['file_name']}",
                "file_name": result["file_name"],
                "num_chunks": num_added,
                "total_documents": self.vector_db.get_document_count()
            }
            
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
                "error": str(e)
            }
    
    def ingest_multiple_files(self, file_paths: List[str]) -> dict:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå
        
        Args:
            file_paths: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ paths
            
        Returns:
            dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        """
        results = []
        total_chunks = 0
        
        for file_path in file_paths:
            result = self.ingest_file(file_path)
            results.append(result)
            if result.get("success"):
                total_chunks += result.get("num_chunks", 0)
        
        success_count = sum(1 for r in results if r.get("success"))
        
        return {
            "success": success_count > 0,
            "message": f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {success_count}/{len(file_paths)} ‡πÑ‡∏ü‡∏•‡πå",
            "total_chunks": total_chunks,
            "results": results
        }
    
    # =========================================================================
    # RAG Chat (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
    # =========================================================================
    
    def chat(self, question: str, prompt_type: str = "qa") -> dict:
        """
        RAG Chat: ‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        
        üéØ Flow:
        1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Retrieval)
        3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏ó‡∏µ‡πà‡∏°‡∏µ context
        4. ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM (LLM ‡πÄ‡∏´‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á context ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°)
        5. LLM ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å context
        
        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            prompt_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó prompt (qa, summary, term, analysis, thai)
            
        Returns:
            dict: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        """
        try:
            # 1. Check if we have documents
            print(f"\nüí¨ Chat query: '{question[:50]}...'")
            
            if not self.retriever.has_documents():
                print("‚ö†Ô∏è No documents in database!")
                return {
                    "success": False,
                    "answer": NO_CONTEXT_PROMPT,
                    "sources": [],
                    "has_context": False
                }
            
            # 2. Retrieve relevant documents
            print(f"üîç Retrieving {NUM_RELEVANT_DOCS} relevant documents...")
            docs = self.retriever.retrieve(question, k=NUM_RELEVANT_DOCS)
            
            if not docs:
                print("‚ö†Ô∏è No relevant documents found!")
                return {
                    "success": False,
                    "answer": LOW_RELEVANCE_PROMPT.format(question=question),
                    "sources": [],
                    "has_context": False
                }
            
            print(f"‚úÖ Found {len(docs)} relevant documents")
            
            # 3. Format context
            context = self.retriever.format_context(docs)
            
            # Debug: Show context preview
            print(f"üìã Context length: {len(context)} chars")
            print(f"   Context preview: '{context[:300].replace(chr(10), ' ')}...'")
            
            # Extract sources for reference
            sources = []
            for doc in docs:
                sources.append({
                    "source": doc.metadata.get("source", "unknown"),
                    "page": doc.metadata.get("page", "?")
                })
            
            # 4. Get prompt template
            prompt_template = get_prompt_template(prompt_type)
            prompt = ChatPromptTemplate.from_template(prompt_template)
            
            # 5. Create chain and invoke LLM with retry
            import time
            
            print("ü§ñ Sending to LLM...")
            chain = prompt | self.llm | StrOutputParser()
            
            max_retries = 3
            response = None
            
            for attempt in range(max_retries):
                try:
                    response = chain.invoke({
                        "context": context,
                        "question": question
                    })
                    break  # Success, exit loop
                except Exception as e:
                    print(f"‚ö†Ô∏è LLM attempt {attempt + 1} failed: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(3)  # Wait before retry
                    else:
                        raise
            
            print(f"‚úÖ Got response: {len(response)} chars")
            
            # 6. Add disclaimer
            final_response = format_response_with_disclaimer(response)
            
            return {
                "success": True,
                "answer": final_response,
                "sources": sources,
                "has_context": True,
                "num_sources": len(sources)
            }
            
        except Exception as e:
            return {
                "success": False,
                "answer": f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
                "sources": [],
                "error": str(e)
            }
    
    def chat_simple(self, question: str) -> str:
        """
        RAG Chat ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ - ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        
        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            
        Returns:
            str: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        """
        result = self.chat(question)
        return result.get("answer", "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î")
    
    # =========================================================================
    # Document Summarization
    # =========================================================================
    
    def summarize(self, source_name: str = None) -> dict:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        
        Args:
            source_name: ‡∏ä‡∏∑‡πà‡∏≠ source ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ (None = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
            
        Returns:
            dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ
        """
        try:
            # Get all relevant chunks
            if source_name:
                query = f"‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á {source_name}"
            else:
                query = "‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"
            
            # Retrieve more docs for summarization
            docs = self.retriever.retrieve(query, k=10)
            
            if not docs:
                return {
                    "success": False,
                    "summary": "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ",
                    "sources": []
                }
            
            # Format context
            context = self.retriever.format_context(docs)
            
            # Use summary prompt
            prompt = ChatPromptTemplate.from_template(LEGAL_SUMMARY_PROMPT)
            chain = prompt | self.llm | StrOutputParser()
            
            summary = chain.invoke({"context": context})
            
            # Get sources
            sources = list(set(
                doc.metadata.get("source", "unknown") for doc in docs
            ))
            
            return {
                "success": True,
                "summary": summary,
                "sources": sources,
                "num_chunks_used": len(docs)
            }
            
        except Exception as e:
            return {
                "success": False,
                "summary": f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
                "error": str(e)
            }
    
    # =========================================================================
    # System Status
    # =========================================================================
    
    def get_status(self) -> dict:
        """
        ‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
        
        Returns:
            dict: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö
        """
        ollama_status = self.llm_client.get_status()
        db_stats = self.vector_db.get_stats()
        
        return {
            "ollama": ollama_status,
            "vector_db": db_stats,
            "has_documents": self.retriever.has_documents(),
            "sources": self.retriever.get_sources()
        }
    
    def get_sources(self) -> List[str]:
        """
        ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ sources ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Returns:
            List[str]: ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ sources
        """
        return self.retriever.get_sources()
    
    def get_document_count(self) -> int:
        """
        ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô documents
        
        Returns:
            int: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô documents
        """
        return self.vector_db.get_document_count()
    
    # =========================================================================
    # Data Management
    # =========================================================================
    
    def clear_all_data(self) -> dict:
        """
        ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Returns:
            dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏•‡∏ö
        """
        try:
            success = self.vector_db.clear_database()
            return {
                "success": success,
                "message": "‚úÖ ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" if success else "‚ùå ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
            }
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"
            }
    
    def delete_source(self, source_name: str) -> dict:
        """
        ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° source
        
        Args:
            source_name: ‡∏ä‡∏∑‡πà‡∏≠ source ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö
            
        Returns:
            dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏•‡∏ö
        """
        try:
            success = self.vector_db.delete_by_source(source_name)
            return {
                "success": success,
                "message": f"‚úÖ ‡∏•‡∏ö {source_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à" if success else f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö {source_name}"
            }
        except Exception as e:
            return {
                "success": False,
                "message": f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"
            }


# Factory function
def get_engine() -> LegalAIEngine:
    """
    Get singleton instance of LegalAIEngine
    
    Returns:
        LegalAIEngine: Singleton instance
    """
    return LegalAIEngine()

# -*- coding: utf-8 -*-
"""
File Loader for Legal AI
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (PDF, TXT, DOCX)

Adapted from open-source RAG architecture
"""

import shutil
from pathlib import Path
from typing import List, Optional

from langchain_community.document_loaders import (
    PDFPlumberLoader,
    TextLoader,
    Docx2txtLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.config import (
    DATA_DIR,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    SUPPORTED_EXTENSIONS
)
from ingestion.text_cleaner import TextCleaner


class FileLoader:
    """
    ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
    
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö:
    - PDF (‡πÉ‡∏ä‡πâ PDFPlumber)
    - TXT
    - DOCX
    """
    
    def __init__(self):
        self.data_dir = DATA_DIR
        self.text_cleaner = TextCleaner()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            add_start_index=True,
            separators=["\n\n", "\n", "„ÄÇ", ".", " ", ""]
        )
    
    @staticmethod
    def is_supported(file_path: str) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            
        Returns:
            bool: True ‡∏ñ‡πâ‡∏≤‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
        """
        ext = Path(file_path).suffix.lower()
        return ext in SUPPORTED_EXTENSIONS
    
    def save_upload(self, file_path: str) -> Path:
        """
        Copy ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á storage
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            
        Returns:
            Path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà copy ‡πÑ‡∏ß‡πâ
        """
        source = Path(file_path)
        if not source.exists():
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}")
        
        target = self.data_dir / source.name
        shutil.copy2(source, target)
        
        return target
    
    def load_document(self, file_path: str) -> List[Document]:
        """
        ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            
        Returns:
            List[Document]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ documents
        """
        file_path = str(file_path)
        ext = Path(file_path).suffix.lower()
        
        print(f"üìñ Loading document: {file_path}")
        print(f"   Extension: {ext}")
        
        if ext == ".pdf":
            loader = PDFPlumberLoader(file_path)
        elif ext == ".txt":
            loader = TextLoader(file_path, encoding="utf-8")
        elif ext == ".docx":
            loader = Docx2txtLoader(file_path)
        else:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {ext}")
        
        documents = loader.load()
        
        # Debug: Show what was loaded
        print(f"   üìÑ Loaded {len(documents)} pages/documents")
        for i, doc in enumerate(documents[:3]):  # Show first 3
            content_preview = doc.page_content[:200].replace('\n', ' ')
            print(f"   Page {i+1}: {len(doc.page_content)} chars - '{content_preview}...'")
        
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        ‡πÅ‡∏ö‡πà‡∏á documents ‡πÄ‡∏õ‡πá‡∏ô chunks
        
        Args:
            documents: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ documents
            
        Returns:
            List[Document]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ chunks
        """
        chunks = self.text_splitter.split_documents(documents)
        return chunks
    
    def add_metadata(self, chunks: List[Document], source_name: str) -> List[Document]:
        """
        ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
        
        Args:
            chunks: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ chunks
            source_name: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á
            
        Returns:
            List[Document]: chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
        """
        for i, chunk in enumerate(chunks):
            chunk.metadata["source"] = source_name
            chunk.metadata["chunk_index"] = i
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° page number ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if "page" not in chunk.metadata:
                chunk.metadata["page"] = 1
        
        return chunks
    
    def clean_chunks(self, chunks: List[Document]) -> List[Document]:
        """
        ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î text ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
        
        Args:
            chunks: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ chunks
            
        Returns:
            List[Document]: chunks ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß
        """
        cleaned_chunks = []
        for chunk in chunks:
            cleaned_content = self.text_cleaner.clean(chunk.page_content)
            if cleaned_content.strip():  # ‡∏Ç‡πâ‡∏≤‡∏° chunks ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á
                chunk.page_content = cleaned_content
                cleaned_chunks.append(chunk)
        
        return cleaned_chunks
    
    def load_and_split(self, file_path: str, clean: bool = True) -> List[Document]:
        """
        ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô chunks (Main function)
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            clean: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î text ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            
        Returns:
            List[Document]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ chunks ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
        """
        # 1. Load document
        documents = self.load_document(file_path)
        
        # 2. Split into chunks
        chunks = self.split_documents(documents)
        
        # 3. Add metadata
        source_name = Path(file_path).name
        chunks = self.add_metadata(chunks, source_name)
        
        # 4. Clean text (optional)
        if clean:
            chunks = self.clean_chunks(chunks)
        
        return chunks
    
    def process_file(self, file_path: str) -> dict:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£: Copy ‚Üí Load ‚Üí Split ‚Üí Clean
        
        Args:
            file_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            
        Returns:
            dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        """
        try:
            # 1. Validate
            if not self.is_supported(file_path):
                return {
                    "success": False,
                    "error": f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ",
                    "chunks": []
                }
            
            # 2. Copy to storage
            saved_path = self.save_upload(file_path)
            
            # 3. Load and split
            chunks = self.load_and_split(str(saved_path))
            
            return {
                "success": True,
                "file_name": saved_path.name,
                "saved_path": str(saved_path),
                "num_chunks": len(chunks),
                "chunks": chunks
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "chunks": []
            }
    
    def get_uploaded_files(self) -> List[dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß
        
        Returns:
            List[dict]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå
        """
        files = []
        for file_path in self.data_dir.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                files.append({
                    "name": file_path.name,
                    "path": str(file_path),
                    "size_bytes": file_path.stat().st_size,
                    "extension": file_path.suffix.lower()
                })
        
        return files
